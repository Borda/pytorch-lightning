apiVersion: elastic.pytorch.org/v1alpha1
kind: ElasticJob
metadata:
  name: s-%(SHA)s-%(PYTHON_VERSION_DASH)s-%(PYTORCH_VERSION_DASH)s-e
  namespace: elastic-job
spec:
  # Use "etcd-service:2379" if you already apply etcd.yaml
  rdzvEndpoint: etcd://%(TCP_ADDRESS)s/s-%(SHA)s-%(PYTHON_VERSION_DASH)s-%(PYTORCH_VERSION_DASH)s-e
  minReplicas: 2
  maxReplicas: 2
  replicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: ExitCode
      template:
        apiVersion: v1
        kind: Pod
        spec:
          containers:
            - name: s-%(SHA)s-%(PYTHON_VERSION_DASH)s-%(PYTORCH_VERSION_DASH)s-e
              image: pytorchlightning/pytorch_lightning:base-cuda-py%(PYTHON_VERSION)s-torch%(PYTORCH_VERSION)s
              imagePullPolicy: Always
              command:
              - bash
              - -ce
              - |
                git clone https://github.com/borda/pytorch-lightning.git /repo
                cd /repo
                git fetch --all
                git checkout %(SHA)s
                pip install -e .
                pip install torchelastic
                python -m torchelastic.distributed.launch --nproc_per_node=%(NUM_GPUS)s --rdzv_id=s-%(SHA)s-%(PYTHON_VERSION_DASH)s-%(PYTORCH_VERSION_DASH)s-e --rdzv_backend=etcd --rdzv_endpoint=%(TCP_ADDRESS)s ./test_multi_nodes/test_multi_nodes_gpu.py --num_nodes=2 --gpus=1 --accelerator=ddp --max_epochs 2
                echo "\n||| END PYTEST LOGS |||\n"
              resources:
                limits:
                  nvidia.com/gpu: %(NUM_GPUS)s
